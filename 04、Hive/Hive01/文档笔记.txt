课前回顾:
1) 自定义输出类: 分文件夹的操作
	1.1) 创建一个类, 继承  FileOutPutFormat  
	1.2) 重写 getRecordWriter()方法
		1.2.1: 自定义一个 RecordWriter 类(决定如何读取文件), 继承 RecordWriter 
		1.2.2: 重写 write(key  value)  close()

2) MapReduce中分组操作:  
	分组:  在同一个分区, 对相同的key的value值进行合并操作形成一个集合过程
	自定义分组: 
		2.1) 创建一个类, 让这个类继承 writableCompar ....
		2.2) 编写无参的构造方法: 通过 super() 指定参数的类型  和 是否要创建这个类型
		2.3) 重写compare(writablecompartable a , writablecompartable b) ; k2
3) job的串联:  
4) MapReduce中参数的优化:  
5) yarn集群的架构和原理 :  yarn的提交流程	
6) RM  和 NM的功能介绍:  RM  和 NM  功能是什么
7) appMaster介绍: 功能 执行流程
8)  yarn的三种资源调度器:  FIFO   capacity scheduler  fair scheduler
9) yarn的多租户的配置:  实现多个用户的资源的隔离操作
10)  hadoop高可用


今日内容:  hive
1) 数据仓库的基本介绍  : 
2) hive的基本概念
3) hive的安装部署 : 
4) hive的交互窗口:  
5)  hive的DDL语句 :  数据库操作语言   
6) hive的DQL语句:  数据库查询语言 



1. 数据仓库的基本介绍: Data Warehouse 存储数据的仓库, 主要的目的是为了面向于分析(数据分析), 为
	企业提供决策支持 以及数据报表的产生
	
	
	数据仓库:  数据仓库本身不生产数据, 同时也不消耗数据
		数据来源于外部, 同时给外部人进行使用
	粮仓 :
	
	数据仓库的特点  :  
		1.1) 面向于主题: 分析的内容(需求)
		1.2) 集成性 : 来源地有很多, 将很多业务系统中历史数据导入到数据仓库
				保险公司: 统计今年一共卖了多少份保险
		1.3) 非易失性 :  不可更新性  在数据仓库中存储的数据都是历史的数据, 都是已经发生过的数据
					一般数据仓库是不允许修改数据
		1.4) 时变性:  随着时间的推移, 原有的分析方案无法满足现有的分析系统, 需要改变分析方式
					对数据也要进行更新(添加最近发生过的数据)

2) 数据仓库 和 数据库有什么区别 :  OLTP  和 OLAP 区别

数据库:  OLTP 联机事务处理
	数据库 和 业务系统之间关系, 通常对数据进行增删改查操作, 一般都要求响应时间不能太长
		支持事务性操作, 
		数据库在设计的时候, 尽可能避免出现冗余数据
		数据库是为捕获数据而设计
		数据库是面向于事务的设计
数据仓库:  OLAP  联机分析数据  
	一般针对某些主题的历史数据进行分析，支持管理决策。
	数据仓库在设置的时候, 有时候专门对数据进行 冗余, 方便查询操作
	数据仓库是为分析数据而设计
	
数据仓库的存在, 不是为了替代数据库,  也不会为了当做一个大型的数据库	

数据库 是用来存储业务系统中数据(最近发生的一些数据)
数据仓库:  存储历史数据(早已发生过的数据)



数据仓库的分层模型: 

	源数据层(ODS) : 将业务系统中数据导入的数据仓库中 不会对数据做任何的处理, 作为一张临时表数据
	数据仓库层(DW) : 对源数据层中数据进行数据的清洗工作(将不规则的数据转换为规则的数据),将清洗
		过的数据变为干净的数据, 将干净的数据存储到另一个表(合并表过程), 在整个表的基础上, 可以数据
			进行分析处理
	数据应用层(APP) :  将数据的结果进行保存, 展示处理....


	数据:  将数据从原始端抽取出现, 对数据进行清洗转化, 将数据在重新的装载到另一个表结构中, 然后开始进行分析展示处理
		抽取, 转化  加载的过程:  ETL(ETL工程师)


数据仓库中元数据的管理 : 元数据是一个非常宽泛的定义, 在哪里都有元数据
	数据仓库中的元数据:  模型的定义(库模式, 表模式), 各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态, 数据清洗的规则
		除了本身的数据以外, 其他都是元数据
	对数据仓库中元数据如何管理:一般将元数据放置在数据库中, 由数据库来进行保存元数据信息
		由数据仓库的软件对数据库中存储元数据进行管理
	
	元数据一般分为两大类: 
		技术性元数据 : 主要服务于 开发 与测试人员
		业务元数据 :   主要服务于 产品人员


hive :  Hive是基于Hadoop的一个数据仓库工具 大数据中用来做数据仓库的工具, 可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。
	没有hadoop, 就不能有hive	
	本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储， 
	hive仅仅提高了可以使用sql的客户端， 然后将sql转换为MapReduce
	
	为什么要hive : 
		不需要写MapReduce了(MapReduce编写起来有一点复杂的), 只需要编写SQL语句(hql  hive的SQL)
		
		会SQL的人, 肯定比会大数据的人多  
	
		在工作中, 使用MapReduce主要进行数据的清洗工作
		使用SQL, 主要是进行数据分析工作: 大部分的需求, 都可以使用sql来实现分析过程
		


hive的架构:
	用户接口:  接收SQL语句
	解析器, 编译器, 优化器, 执行器:  将SQL转换MapReduce, 并提交给yarn集群
	
hive 和 hadoop关系:  hive 严重依赖于hadoop, 如果没有hadoop, 那么hive也就无法执行
	如果要使用hive操作, 必须先启动hadoop
	
	
hive 和传统数据库对比:  
	hive主要是用于海量数据的离线数据分析

hive的数据存储: hive将数据存储到hdfs中
	hive本身自己没有数据类型, 沿用hdfs的数据类型:  Text ,  sequence  ORC RCFILE
	hive在创建表的时候, 只需要执行列的分隔符号和行的分隔符号即可 , 自动映射表数据
	hive所支持的数据模式: DB、Table，External Table，Partition，Bucket。
		hive四种表模式:  管理表(内部表), 外部表, 分区表, 桶表
 
hive安装:  
	1) 下载(安装包) :  	CDH5 中 下载 5.14.0  
			hive-1.1.0-cdh5.14.0/
	2) 上传到linux中, 将压缩包记性解压即可, 解压后即可使用 (拆箱即用)

	使用hive的方式有二种: 
		一种使用hive自带数据库进行元数据存储:  不需要修改任何配置文件
			在哪里启动, 就会在哪里创建一个hive自带元数据库(derby), 导致元数据存储了很多份, 存在数据脑裂的问题
		
			注意: 测试完第一种后, 将解压包, 重新解压一份新的, 来演示下一种方案
		一种使用外部数据库来进行元数据存储 : 修改修改配置文件  --推荐
			目前使用的是第三方的数据库(mysql)
		
hive的交互窗口:   
		1) 使用shell方式交互:  
			bin/hive  操作    ---- 这种方式, 如果mysql中没有hive库, 在第一次初始化的时候 会加载hive库
			注意: 必须使用第一种启动测试一次, 保证hive库是可以出来的, 否则后续的交互方式必须基于hive库 ,如果没有, 不会自动创建
		2) 使用hive的jdbc的服务 : 
			单独启动一个服务器, 单独启动一个客户端
			通过客户端连接对应服务端即可
			2.1) 先启动一个服务端:  hiveserver2
				前端启动:  bin/hive --service hiveserver2 启动后占用前端的窗口, 导致无法继续操作
				后端启动:  nohup  bin/hive --service hiveserver2  &
					nohup (将日志信息打印到 nohup.out中)   &  组合后, 实现将程序挂载到后端运行

			2.2) 启动一个客户端:   beeline来连接
				先  bin/beeline  执行
				执行后, 输入:  !connect jdbc:hive2://node03:10000 输入用户名和密码
					用户名: 保证和 启动hadoop的用户名一致(或者使用最高权限的用户名)
					密码:  随意写, 或者不填写
		3) 使用hive的命令的方式来连接hive : 不需要登录hive, 即可操作hive数据库
			命令一:  hive -e "SQL语句";
			命令二 : hive -f sqlFilePath
			适合于在开发环境下, 对于某些分析的需求, 这些需求都是每天都要执行, 或者每个小时都会执行
				一般会将SQL语句写入到一个SQL的脚本文件中, 当需要使用这条的时候, 直接执行SQL脚本就可以了
					同时也可以通过一些定时的任务来定时执行脚本, 此时就需要使用这种连接交互方式
			
hive的DDL语句: 对库 和 表的定义操作语言
	对库操作:   
		创建数据库:  create database [if not exists] 数据库名称;   *****
		使用数据库:  use  数据库;  ***** 
		修改数据库(不能修改数据库名称):  alter  database  数据库名称  set  dbproperties('createtime'='20180611');
		查看数据的信息:  
			desc database  数据库名称;  *****
			desc database extended  数据库名称;  查看详细的信息
		删除数据库:  *****
			drop database  数据库名称 ;  会将hdfs中对于这个目录信息删除掉, 但是如果这个库里面有表, 那么无法删除数据库
			drop  database  myhive  cascade; 会将hdfs中对于这个目录信息删除掉, 如果有表, 连表一起给删除了
						
	对表的操作 :  
	CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name  
	   [(col_name data_type [COMMENT col_comment], ...)] 
	   [COMMENT table_comment] 
	   [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
	   [CLUSTERED BY (col_name, col_name, ...) 
	   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 
	   [ROW FORMAT row_format] 
	   [STORED AS file_format] 
	   [LOCATION hdfs_path]
	
	EXTERNAL(external) :  如果在创建表的时候, 添加了这个关键词, 表示这是一个外部表, 如果不添加, 表示是一个管理表(内部表)
	
	PARTITIONED BY (partitioned by):  如果在创建表的时候, 添加了这个关键词, 表示是一个分区表
	CLUSTERED BY(clustered by into 3 buckets)   :  如果在创建表的时候, 添加了这个关键词, 表示是一个 分桶表  cluster
	SORTED BY  (sorted by)    : 指定要以哪个字段或者那几个字段对数据进行排序操作
	INTO num_buckets BUCKETS :   和 CLUSTERED BY 组合使用, 指定有几个桶
	ROW FORMAT  (row format delimited fields terminated by '\t') :  指定 字段和字段之间的分隔符号
	STORED AS file_format (stored as textfile) : 指定 文件数据的类型, 是 textFile , 还是sequenceFile.....
	LOCATION hdfs_path  :  指定从那里加载数据
	
		
		对管理表(内部表)操作 :  不需要添加  EXTERNAL   *****
			1) 创建一个表 :  
				最简单表:  create table stu(id int , name string);  默认字段的分隔符号  \001
				添加数据:  insert into stu values (1,'zhangsan');  ---了解, 实际开发不会使用的
						因为, 这种添加方式, 每一次执行都会产生一个小文件
				查看所有表:  show tables ;
			
			2) 创建一个表:指定分隔符号
				create  table if not exists stu2(id int ,name string)  
				row format delimited fields terminated by '\t'   ---字段与字段之间使用\t分隔
				stored as textfile          ----数据文件的类型是 普通文本类型
				location '/user/stu2';     --- 从哪里加载数据(hdfs的路径)

			3) 根据查询的结果来创建表:
				create table 表名 as 查询语句;
			4) 根据已经存在的表结构创建表 :
				create table 表1 like 表2;
			5) 查询表的详细的结构 : 
				desc formatted  stu2; 
			
		管理表的特点:  hive认为管理表, 是我自己独有的表, 在删除这个表的时候, 会将和这个表的里面数据一并全部删除了
			将分析后的结果信息保存到管理表中 :  drop table stu3;
			
		外部表:  添加  EXTERNAL(external)   *****
			外部表特点:  hive认为外部表中数据, 不是hive独有的数据, 在删除这个表的时候,不会将这个表的数据给删除了
				共用的数据, 在构建表模式的时候, 必须使用外部表来构建, 不要使用内部表
			对于外部表 和内部表在操作上基本一样的, 只是多了一个关键词:  EXTERNAL
			
			
			从本地系统向外部表添加数据 :  
				load data  local inpath  "filePath"  into table  表名 ; 
					本质上执行一条: hdfs  dfs -put  本地路径   hdfs的路径(表加载路径下)
			从hdfs想外部表添加数据:
				load data   inpath  "hdfsFilePath"  into table  表名 ; 
					或者使用:  hdfs dfs  -mv 
		分区表:  分文件夹   关键词 PARTITIONED BY  (partitioned by)   *****
			对于hive来讲, 独立的表模型只有二个:  外部表, 内部表	
				分区:  外部的分区表 和 内部的分区表
				提高的查询的效率, 将不同数据按照分区进行划分, 当查询某一条数据的时候, 只要带上分区字段, 只需要查询这个分区文件夹下面的文件中数据即可
			创建分区表 :
				创建只有一个分区字段的分区表:
					create table score(s_id string,c_id string, s_score int) 
					partitioned by (month string) row format delimited fields terminated by '\t';
				创建有多个分区字段的分区表:
					create table score2 (s_id string,c_id string, s_score int) 
					partitioned by (year string,month string,day string) 
					row format delimited fields terminated by '\t';
			
			加载数据:  
				往一个分区加载:
				load data local inpath '/export/servers/hivedatas/score.csv' into table score 
					partition (month='201806');
				往多个分区加载数据:
					load data local inpath '/export/servers/hivedatas/score.csv' into table score2 
partition (year="2019",month='09',day='04');
			查看分区有几个:  show  partitions  表名;
			添加分区字段:  
				alter table score add partition(month='201805');
				alter table score add partition(month='201804') partition(month = '201803');
				本质上就是在表的hdfs的目录下, 创建一级或者多级的目录
			删除分区: 
				alter table score drop partition(month = '201806');
			请问": 在创建表的时候, 设置分区字段, 是否已经有分区了呢????  没有  
			
		分桶表:  分文件的  和 MapReduce中分区是类似的  : 提高查询的效率的    *****
			设置分桶表, 将那个字段 或者那几个字段当做 k2 来往下发过程
				也不是独立的表模型: 外部的分桶表 和 内部的分桶表
			首先第一步: 开启分桶方案
				set hive.enforce.bucketing=true;
			第二步: 设置reduce的数量:
				set mapreduce.job.reduces=3;
			
			第三步: 创建桶表:  CLUSTERED BY (col_name, col_name, ...)   INTO num_buckets BUCKETS
				create table course (c_id string,c_name string,t_id string) 
clustered by(c_id) into 3 buckets row format delimited fields terminated by '\t';
				桶表在加载数据的时候, 必须执行MapReduce, 否则无法加载 :  insert  overwrite 
				
				给桶表加载数据的时候, 一般是先创建一个临时表, 将所有的数据导入的临时表当中, 然后使用 insert  overwrite 
					从这个表将数据查询出来, 添加到 分桶表中
					insert overwrite table 桶表 select * from 普通表 cluster by(c_id);
			
		hive的四种表模式:  内部表 外部表  分区表  桶表
			独立的表模式:  
				内部表 和 外部表:  是否将hdfs中存储数据删除
					
		修改表 和 删除表:  
			表的重命名:  alter  table  old_table_name  rename  to  new_table_name;
			增加/修改列信息 : 
				1）查询表结构
					hive (myhive)> desc score5;
				2）添加列
					hive (myhive)> alter table score5 add columns (mycol string, mysco string);
				3）查询表结构
					hive (myhive)> desc score5;
				4）更新列
					hive (myhive)> alter table score5 change column mysco mysconew int;
				5）查询表结构
					hive (myhive)> desc score5;
			删除表: drop table score5;
			
		如何向hive表中加载数据: 需要记忆  *****
			通过load方式:   外部表, 内部表, 分区表
				load data [local] inpath 'filePath|hdfsFilePath' 
					[overwrite] into table 表名 [partition(month='201806')];
			通过insert overwrite 加载数据: 外部表, 内部表, 分区表 分桶表  执行效率慢一点
				insert overwrite table 表名 [partition(month = '201806')] 查询语句  [cluster by(c_id);];
			
			使用import进行导入数据：  
				import table techer2 from '/export/techer';
			
		hive如何导出数据：  **
			使用insert导出:
				导出的目录下: 如果有local表示导出到本地, 如果没有, 导出到hdfs上的路径
				 insert overwrite [local] directory '/export/servers/exporthive' 
					row format delimited fields terminated by '\t' collection items terminated by '#' select * from student;
				
				使用export将表数据导出到hdfs的目录上:
					export table score to '/export/exporthive/score'; 
				
				使用shell命令方式导出:
					bin/hive -e "select * from myhive.score;" > /export/servers/exporthive/score.txt
							将结果重定向到一个文件中操作
					
		清空表数据: 只能清空内部表, 内部的分区表, 内部的分桶表
			truncate table score6;
			如果清空的是外部表, 直接抛异常
			
				
				