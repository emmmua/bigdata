课前回顾
1) hadoop发展历史:   2008年将hadoop贡献给apache
	doug cutting
2) hadoop的历史版本:  0.x 1.x  2.x  3.x


3) hadoop的三大发行公司: apache ,  Horton works  cloudera (CDH5)
	
4) hadoop的模块组成 :  1.x  和  2.x

5) hadoop的架构模型:  重点搞定 2.x 架构模型

6) hadoop的三种运行方式 :  

7) CDH版本hadoop重新编译: 支持C语言访问接口 实现 C方式数据压缩
8) hadoop集群的搭建
9) hadoop初体验:  hdfs   yarn平台上去运行MapReduce4
10) jobhistory (查看日志) 和 hadoop的垃圾桶

今日内容: hdfs   
1) hdfs的是什么东西
2) hdfs的基本架构
3) hdfs的特性
4) hdfs的命令操作  (会操作)
5) hdfs的高级命令 :  (知道就行)
6) hdfs的block块和副本机制
7) hdfs的读写的流程  (记忆下来)
8) namenode工作机制以及如何管理原数据信息(记忆)
9) datanode工作机制(知道就行)
10) block的手动拼接



1) hdfs :分布式的文件存储系统
	在hadoop中, 支持很多种文件存储系统, 只不过hdfs是使用最多的一种
	FileSystem:抽象类, 在这个抽象类下面有很多的实现类, 来对应不同的文件系统
		localFileSystem : 本地文件系统  (本地系统中自带的文件系统)
		distributeFileSystem : 分布式的文件系统 (hdfs)
		webFileSystem :  浏览器操作文件系统实现类
		harFileSystem:  处理小文件

	如何实现一个分布式的文件系统呢  ? 
		hdfs:  我凑五毛, 你凑5毛,, 共同凑1块钱过程
		hdfs本质上就是一款软件, 数据真实存储在服务器的本地磁盘上
			hdfs将各个服务器的磁盘进行打通管理

2) hdfs的架构:
	namenode和datanode中间有三种重要机制来保证: 心跳机制, 负载均衡, 副本机制
	
	namenode: 管理元数据和对集群管理, 元数据存储在内存当中
	datanode: 存储数据, 数据存储在本地磁盘上

3) hdfs的特点:
	3.1) hdfs的是一个主从的架构: 主节点namenode  从节点 datanode
	3.2) 数据都是分块来存储 : 每一块的大小最多只能 128M
	3.3) 名字空间:  在访问一个文件的时候, 都是通过ip:端口号/访问目录位置
			都有统一的访问路径  
			hdfs://namenodeIp:port(8020)/目录位置
	3.4) namenode,对元数据进行存储和管理: 文件路径, 在那个datanode, 副本数量
	3.5) datanode: 存储数据
	3.6) 副本机制: 默认hdfs的副本数量为 3个  防止数据丢失
	3.7)  一次写入, 多次读取: 不支持文件的修改, 
			修改元数据, 每一个副本也要修改, 延迟很大

4) hdfs的操作命令: hdfs  dfs 操作命令 |  hadoop fs 操作命令

		ls  path :  查看指定目录下的信息
		mkdir  -p  path : 创建文件夹  -p创建多级文件夹
		put  localSrc  hdfsDis    : 将本地文件上传到hdfs中
		moveFromLocal localSrc  hdfsDis   :  将本地移动到hdfs中
		appendToFile  localSrcList   hdfsdis :  将多个文件往hdfs某一个文件追加数据, 如果hdfs中不存在这个文件, 就会先创建, 如果存在就会追加
		get hdfsFilePath  localPath  : 从hdfs将数据下载本地
		getmerge hdfsFilePath  localPath  : 将hdfs中多个文件合并成一个文件下载到本地
		mv srcPath  disPath  : 在hdfs内部, 进行目录之间的移动操作
		rm  [-r |  -f]  filePath  :  删除文件, 如果开启了垃圾桶, 默认会放置垃圾桶
		cp  :  在hdfs内部, 对某一个文件进行复制操作
		cat  : 查看hdfs中文件的内容
		chmod [-R] : 设置文件权限,  -R  递归修改
			目前已经将hdfs中的权限全部关闭了
		chown  [-R]   : 修改用户和用户组的 -R  递归修改
		setrep  : 修改文件的副本的数量, 如果需要修改, 修改的副本数量最多和datanode得节点的数量是一样的

5) hdfs的高级命令 :  hdfs dfsadmin 操作命令
	5.1) 对文件夹的限额配置:  
			对数量的限额 :  hdfs dfsadmin -setQuota 2  dir 
			空间大小限制  :  hdfs dfsadmin -setSpaceQuota 4k /user/root/dir
		其他的管理命令:
			hdfs dfs -count -q -h /user/root/dir1  #查看配额信息
			清空数量的限制:  hdfs dfsadmin -clrQuota 2  dir 
			清空空间大小的限额:    hdfs dfsadmin -clrSpaceQuota 4k dir
	
	5.2) hdfs的安全模式:    hdfs的自动保护
		当hdfs在启动后, 自动进行的安全模式下, 默认时间为30s , hdfs会进行自检(namenode和datanode通信, 检测block块是否是正常的, 已经datanode是否已经都活过来了)
			在安全模式下, 不允许用户执行增删改的操作, 只允许用户执行查询的操作
		同样也可以手动开启和关闭安全模式:
			hdfs  dfsadmin  -safemode  get #查看安全模式状态
			hdfs  dfsadmin  -safemode  enter #进入安全模式
			hdfs  dfsadmin  -safemode  leave #离开安全模式
	
6) hdfs的block块和副本机制
	hdfs将所有的数据都作为block来存储, 不关心文件类型, 一视同仁, 都认为是一个文件
	一个block的大小, 默认最多只能是128M ,在1.x中默认64M
	<property>
        <name>dfs.block.size</name>
        <value>块大小 以字节为单位</value>//只写数值就可以
    </property>
	
	为什么要抽取成block快来存储:
		1) 都以相同的大小来存储, 每一个block即可, 放置在不同的服务器当中
		2)  不需要关系文件类型, 简化了文件系统
		3) 可以对块进行各种各样的备份 处理
	
	如果一个文件的大小是  300M  , 请问需要才分几块:  3块  128+128+44
	
	
	请问44M  最终在hdfs中,占用 128M 呢 , 还是44M :  如果不满足128M. 占用hdfs的空间,实际有多大, 就占用多大
		如果现在有100万个小文件, 都是1Kb, 全部存储都hdfs中:
	

	如果还有一个文件, 129M:   也会被分为两个文件块, 每一个文件块只能是一个文件的文件块,
		不能将多个文件合并在一个文件块中
	
	一个文件就会有一份元数据信息. 这些元数据信息都存储在了namenode服务器的内存中
		如果hdfs中, 有大量的小文件, 就会有大量的元数据信息, 每一份元数据大约占用 120字节
	如果小文件过多, 占用内存元数据也会越多, 这样如果内存一旦满了, hdfs也就无法在进行存储数据了, 即使datanode还有空间
			hdfs将磁盘给打通了, 但是没有把内存打通
	对于hdfs来讲, 存储容量的大小, 取决于内存:  尽可能的存储大文件的数据
	
	
	总结:  hdfs适合于存储大文件, 不适合存储小文件
		如果小文件怎么办:  
			1) 小文件合并为大文件:
			2) Hbase:  专门用来存小文件
			
			
	hdfs的文件权限:  hdfs是一个软件 权限比较弱
		防止好人做错事，而不是阻止坏人做坏事
		HDFS相信你告诉我你是谁，你就是谁 (单纯)
	
	hdfs的副本机制: 默认是 3个
	
		<property>
                <name>dfs.replication</name>
                <value>3</value>
        </property>
		
		
请问这个三个副本, 随意存储的吗? 如果不是, 你觉得应该怎么存储?

	1) 网络拓扑关系: 寻找最近的节点对服务器进行排序
	2) 机架感知原理 :  从一个机架找二台, 然后从另一个机架在找一台存储
			
	


hdfs的写入数据的过程 :  记忆下来

总结写入数据的流程:  
	1) client请求namenode, 执行上传的操作
	2) namendoe在接收到请求后, 要判断:  权限 文件是否已经存在
		不管判断哪个, 只要不符合, 都会报错
			如果都符合, 返回可以上传文件
	3) client对文件进行切割处理 , 默认按照一个block为 128M 进行切分
	
	4) client会再次请求namenode, 寻找第一个block应该存储在那些datanode服务器列表中
	5) namenode,根据 网络拓扑 和机架感知以及副本机制, 返回datanode服务器列表
	6) client收到服务器列表后, 先获取其中的第一台, 与之关联, 然后由一台和第二台关联, 第二台与第三台关联
		建立整个 pipeline管道体系
	7) client就开始发送数据, 数据是以 package(数据包 64kb)  当第一台接收后, 然后第一台在发送给第二台, 第二台发送给第三台
		还会建立应答体系, 每一台接收数据后, 都需要给应答体系做一个响应
		
	8) 当第一个block发送完成后, client会在再次请求第二个block应该存储在那些datanode服务器列表中
		再次从第六步开始执行 即可


总结hdfs的读取的流程:   记忆下来
	1) client请求namenode, 执行读取数据的请求
	2) namenode在接收到请求后, 要判断: 权限 文件是否已经存在
		不管判断哪个, 只要不符合, 都会报错
			如果都符合, 会视情况返回部分或者全部的block的列表地址 (datanode地址)
		 	
	3)  client接收到block的列表地址后, 并发的连接各个block的地址, 读取数据, 在读取完成后, 会进行校验(判断是否全部读取完整)
	4) 如果namenode返回的部分列表, client会再次请求namenode, 获取下一批block的列表地址
		再次从第三步开始执行, 直到将所有block读取完成
		
	5) 将各个block按照顺序将所有block拼接在一起, 组合成一个文件
	


namendoe :  如何来保存元数据信息

	fsImage :  镜像(元数据)   保存可一份比较完整的元数据信息
	edits:   保存一份最近修改的元数据的日志信息
	
	如何获取一份完整的元数据信息:  fsImage + edits
	
	
	<!--存储namenode的fsImage文件目录  ,也是支持编写多个目录的, 注意, 如果是多个目录, 存储数据都是一样的, 相当于备份了多次  -->
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatas</value>
	</property>
	<!--存储namenode的edits文件目录  -->
	<property>
		<name>dfs.namenode.edits.dir</name>
		<value>file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/edits</value>
	</property>
	

	<!--存储snn的fsImage文件目录  -->
	<property>
		<name>dfs.namenode.checkpoint.dir</name>
		<value>file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/snn/name</value>
	</property>
	<!--存储snn的edits文件目录  -->
	<property>
		<name>dfs.namenode.checkpoint.edits.dir</name>
		<value>file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/nn/snn/edits</value>
	</property>


SNN合并的流程:  记下来
	1) snn通知namenode进行切换edits文件
	2) snn将namenode中edits文件和fsImage文件整体放置到snn的本地磁盘
	3) snn将edits文件和 fsimage文件加载到内存当中, 执行合并, 形成一个新的fsImage文件
	4) 将这个新的fsImage发送给NameNode的某一个目录下面, 替换原有文件(新增)

优化: 
	如果在一段时间内没有操作hdfs, 在合并后, 也不会产生新的文件

snn: 看做是一个冷备份	
	确保元数据不会丢失, 
	文件的大小,最多和内存的大小是一样的
	
	snn要中要保留edits和fsImage的原因:  在另一个服务器在保留一份较为完整的元数据,以防止元数据丢失的问题
		一旦丢失后, 可以通过从snn中来恢复元数据, 但是要注意的无法恢复全部的数据(最近一段时间内元数据的是无法恢复)
	
dfs.namenode.checkpoint.period     3600  : 间隔多长时间合并一次元数据,默认是1个小时
dfs.namenode.checkpoint.txns       1000000 :  间隔多长操作次数, 来合并一次元数据信息, 默认是 100w
dfs.namenode.checkpoint.check.period   60	: 做检测, 每隔一分钟, 检查一次操作的次数
	
	
尝试将namenode中元数据整体删除, 尝试使用snn中元数据进行数据恢复:
	1) 停止hdfs
	2) 删除namenode中edits文件和fsiamge文件
	3) 将snn的保存的元数据复制到namenode中, 
	4) 启动namendoe, 文件是否依然是存在的



datanode存储文件目录:

	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:///export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatas,file:///dev/shm/a/</value>
	</property>

	在生产环境下, 这个目录, 不能随意瞎填:  df -lh(确定磁盘的挂载目录), 然后填些目录位置, 如果磁盘有多个, 目录配置也应该有多个
		每一个目录对应着一个磁盘
	 
	当配置了多个后, 扩大一个服务器的存储的容量
	

动态的新增一个节点 :  在不停机的情况下, 动态的添加一个新的datanode  (操作)
	


如果要下线一个datanode, 通常做法:  直接将要下线的节点直接停止即可, 一般情况下, 不会下线服务器



block的手动拼接称为一个完整的文件:  验证文件切分是可以进行合并
	
block存储目录:
/export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatas/current/BP-223294050-192.168.72.141-1566803847781/current/finalized/subdir0/subdir0
	
	
	
http://www.pansoso.com/zh/spring
http://www.pansou.com/


