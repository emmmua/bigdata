课前回顾
1) MapReduce中分区  : 将相同类型的key发往同一个reduce当中
	默认的分区的规则 : (key2.hashCode()&Integer.maxValue())% numRedecerTasks
	默认numRedecerTasks :  1 
		如何修改默认值:  job.setnumRedecerTasks(n)
	如何来自定义分区的规则 :  
		1) 创建一个类, 继承partitioner
		2)  重写 int getPartition(); 给key2 打标识
		3) job.setPartitionClass(自定义的分区类)
	注意: 一旦使用多个分区情况(numRedecerTasks个数大于 1), 必须放置在集群中使用否则是不生效的

2) MapReduce的排序 和 序列化
	序列化:  将结构化的数据转换为 字节数据的过程 (网络传输, 持久化)
	排序: Comparable接口
		如果要实现序列化和排序 :  WritableComaprable
			一个排序的方法:
				当前的值比上参数中的值, 升序 , 参数的值比上当前值就就是倒序排序
			一个序列化的方法
			一个反序列化的方法
				注意: 序列化 和 反序列的顺序是一致的, 使用对应类型来给对应数据进行序列化和反序列化
	
3) MapReduce的计数器:  

4)MapReduce中combiner(规约) :  优化MapReduce的步骤
	减少map端输出的数据量, 减少reduce端拉取数据的网络io量
	局部的合并操作: 对每一个map进行合并操作
	
	如何定义一个combiner的类:  定义的方式和reduce的定义方式是一样的
		job.setCombinerClass(combiner的类)
	
	注意: combinner  输入和输出的类型都是 k2  v2
5)  综合案例 :  

今日内容  -----MR
	1) MapReduce的运行机制 ----理解
			mapTask的运行机制
			reduceTask的运行机制
	2) shuffle阶段的数据压缩机制
	
	3) MapReduce的join的实现
	4) 社交粉丝数据分析 :  求共同好友
	5) 倒排索引建立:  wordCount
	6) 自定义输入
	7) 自定义输出



1) MapReduce运行机制: 
	1.1) mapTask的并行度 :  mapTask运行多个, 一起来处理数据过程, mapTask的数量和block的数量是一致的
		block:  是hdfs中对文件的定义
		fileSplit:  文件的切片, 默认情况下, 文件的切片的大小 和 hdfs中block的大小是一致的
			mapreduce.input.fileinputformat.split.minsize
			mapreduce.input.fileinputformat.split.maxsize : 默认取Long类型最大值
		
		
		mapTask的数量 和 文件切片的数据是一致的, 而文件切片的大小 一般和 block的大小是一致的

	1.2) mapTask的运行机制 :
		1.2.1) 读取数据, 使用TextInputFormat, k1 v1, 数据读取一行, 就会调用一次map的逻辑
		1.2.2) 自定义map的逻辑, 将 k1 和 v1转换为 k2  和 v2, 每执行一次, 就会往外输出一些内容
		1.2.3)  执行  分区的逻辑: (自定义的分区逻辑, 默认的分区逻辑), 主要的目的, 就是为了给key2进行打分区号标识
		1.2.4) 将这些数据缓存到 环形缓存区中, 环形缓冲区默认的大小是100M , 有一个临界值0.8, 当达到这个临界值的时候, 
			会启动一个溢写的线程, 将80%的数据写出一个本地的临时的文件中, 剩余20%还可以并行的存储数据 当溢写的线程执行
				完成后, 剩余的80%空间也是重复的利用, 达到循环使用的过程, 
		1.2.5) 最终当map执行完成后 , 将环形缓存区中数据全部的溢写出一个临时本地文件中 
				将临时文件进行 merge(合并)操作, 最终一个mapTask只能产生一个最终结果文件, 等待reduce进行数据的拉取
		
	1.3) reduceTask的运行机制:
		1.3.1) 执行copy: 每一个reduceTask都会启动一个copy的线程, 到所有的mapTask执行完成的结果文件中拷贝属于自己的数据(核心根据分区)
		1.3.2) 将数据先存储到内存当中, 如果内存当中达到一定阈值后, 还会启动一个溢写的线程, 将数据溢写本地的临时文件中
				这个过程和mapTask的溢写的过程非常类似, 但是要比mapTask更加灵活
		1.3.3) 在溢写的过程中 也会产生多个临时的文件, 当copy线程完成后, 就会对临时文件进行一次合并操作, 最终形成一个完整结果文件
		1.3.4) 对结果文件再次进行排序操作, 保证在一个reducerTask中是有序的 , 如果reduceTask只有一个, 此处就是一个全局排序, 如果不是, 依然是一个局部排序
		1.3.5) 对排序好的结果文件, 进行分组的操作, 将相同的key的value执行进行合并成为一个集合, 合并一次就会调用一次reduce的逻辑
		1.3.6) reduceTask执行内部的逻辑, 将k2 和 v2 转换为 k3  和 v3  输出即可
		1.3.7) 输出后, 通过TextOutPutFormat将数据写出的本地磁盘即可

	shuffle是整个MapReduce中核心: 
		同时如果要对MapReduce进行优化, 主要优化的部分就是shuffle过程
		
		
	shuffle的范围:  从 map输出数据开始, 到reduce逻辑接收到数据之前, 数据shuffle的阶段



2. 数据压缩:  
		核心目的: 节省磁盘的空间 , 减少网络传输的io量

		MapReduce中, 可以对那些位置进行数据的压缩????
			2.1) map最终输出文件可以进行压缩 :  减少网络传输的io量
			2.2) 对reduce输出的文件数据做压缩  : 节省磁盘的空间 
		请问:  hadoop支持压缩吗 ?????  支持的
			hadoop checknative :     zlib snappy lz4  bzip2

		应该使用哪种压缩方案:   
			1) 每一种压缩方案 压缩比  
			2) 压缩的效率
			取一个压缩比和效率结合在一起最高的: 性价比比较高
		
		hadoop默认情况下, 是不开启压缩的方式的, 如果要使用压缩的方案, 首先需要进行相关的配置来开启压缩方案:
			压缩: compress
		
			压缩的相关配置信息:
				reduce端压缩配置:
					mapreduce.output.fileoutputformat.compress	false : 默认不开启压缩
					mapreduce.output.fileoutputformat.compress.type	RECORD : 如何进行压缩: [NONE, RECORD  BLOCK] 
						NONE : 不压缩
						RECORD :   按行压缩(只对v3做压缩)
						BLOCK :  按块来压缩 (对 k3  v3 都压缩)
					mapreduce.output.fileoutputformat.compress.codec	org.apache.hadoop.io.compress.DefaultCodec : 使用那种压缩算法来压缩, 一般建议使用snappy
																		org.apache.hadoop.io.compress.SnappyCodec
				map端压缩配置:
					mapreduce.map.output.compress	false : 默认不开启map端的压缩
					mapreduce.map.output.compress.codec	org.apache.hadoop.io.compress.DefaultCodec: 使用哪种压缩算法, 建议使用snappy
														org.apache.hadoop.io.compress.SnappyCodec
		如何开启压缩: 
			1) 永久对全部MapReduce开启压缩:  需要在hadoop的mapred-site.xml中配置 : 一般都不使用
			2) 只针对某一个MapReduce开启压缩:  需要在job中, 使用configuration来开启压缩 : 推荐使用

		一旦开启压缩后， 需要在yarn平台上来执行， 并且数据应该存储在hdfs中


mMapReduce的join的实现:  
	
需求:  有如下的二个文件, 要求将两个文件对应的合并在一起即可

商品表:
p0001,小米5,1000,2000
p0002,锤子T1,1000,3000

订单表: 
1001,20150710,p0001,2
1002,20150710,p0002,3
1002,20150710,p0003,3
	

使用SQL如何写: 
	select * from  product p  right join  orders o on  p.pid = o.pid ; 
	
最终结果:
p0001,小米5,1000,2000   1001,20150710,p0001,2
p0002,锤子T1,1000,3000  1002,20150710,p0002,3
 null                   1002,20150710,p0003,3   

要求使用MapReduce来实现 : 相同类型的key会发往从一个reduce中, 相同key的value值会合并成一个集合

reduce的join实现: 在那一端进行两个表的合并操作, 那就是哪一种join的实现

map阶段 :  k2      v2
		  pid      订单表数据或者商品表 (自定义数据类型, 将两个表的字段都封装进去)

reduce阶段: 相同key的value值会合并成一个集合
			k2       v2
		p0001 :  [ {p0001,小米5,1000,2000} , {1001,20150710,p0001,2} ]

		
			k3                 v3
			自定义数据类型	   nullwritable
		
	感觉有没有一些不好的地方: 
		如果某个商品被很多很多的用户购买了, 比如说 p001这个商品在1000万用户购买, 但是 p0002只有1个用户购物了
			在进行join操作的时候, 分为两个文件来显示不同商品都那些订单:
				会导致一个reduce出现过多的数据, 而一个reduce中却没有数据, 专业说法叫 数据倾斜问题
			 
			使用reduce端的join, 极容易发生数据倾斜的问题 
		
		如果两个表都是大表的情况下, 可以需要使用这种join的实现
		
map端join: 
	适用于某一个文件数据比较少的情况下来实现:  大表 和 小表进行join的情况
	
	商品表其实就是小表 : 可以先将小表的数据先缓存到内存当中去, 
		然后去文件中读取大表中每一条数据, 然后和内存中小表数据进行匹配

	使用块缓存操作:  DistributeCache对象
			DistributeCache.setCacheFile(URI uri , Configuration configuration) ; 添加缓存文件
				这个方法要在 job的run方法创建 job任务之前来设置, 否则不生效, 并且路径必须是hdfs中路径
			
			DistributeCache.getCacheFiles(Configuration configuration) ; 获取所有的缓存文件的
				在哪里需要找到缓存文件, 在哪里获取即可


社交粉丝数据分析:  求共同好友
	需求:  求 俩俩之间有那些共同好友, 展示出所有的共同好友
	好友是单向的还是双向的: 单向的
	A:B,C,D,F,E,O
	B:A,C,E,K
	C:A,B,D,E,I 
	D:A,E,F,L
	E:B,C,D,M,L
	F:A,B,C,D,E,O,M
	G:A,C,D,E,F
	H:A,C,D,E,O
	I:A,O
	J:B,O
	K:A,C,D
	L:D,E,F
	M:E,F,G
	O:A,H,I,J

分析流程， 查看第二个笔记内容



倒排索引 : 	
	需求:  某一个单词, 在那些文件中出现过, 出现了多少次
	
	可以用于提高查询的效率:   统计出某一个单词在一个文件中出现的频次,
		如果频次越高, 说明当用户搜索这个单词的时候, 查询这个文件的可能性越高
	

自定义输入 :

	需求:  使用MapReduce实现小文件合并操作 :  
		在hdfs中, 不建议存储太多的小文件, 因为占用很多的元数据信息
		在MapReduce中, 同样也不建议输入有太多的小文件, 因为每一个小文件都要启动一个mapTask来运行

	目前在某一个目录下, 有5个小文件, 如果使用默认的TextInPutFormat 启动5个mapTask来运行
	
	通过自定义的方式, 编写一个输入类, 让这个类在读取多个文件的时候, 不进行分片处理, 合并成一个文件当做是我们输入
		要求最终结果输出的时候 ,能够区分出是那个文件的数据


	如何自定义的输入类:  抄
		1) 定义一个类, 继承  FileInputFormat , 指定 k1  v1
		2) 重写里面的方法: createRecordReader(),isSplitable()
				isSplitable() : 判断, 对某一个文件, 是否需要进行切分  
		3) 自定义RecordReader:  
				编写一个类, 继承 RecordReader , 泛型就是 k1  v1
				重写 RecordReader中方法
		

自定义输出:















